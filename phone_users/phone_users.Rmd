---
title: "PhoneUsers"
author: "Group D"
date: "2024-04-10"
output: html_document
---

```{r include=FALSE}
# Clear workspace, install library & import data
library("caret")
library("Metrics")
library("corrplot")
library("earth")
library("tree")

set.seed(1234)
```

```{r include=FALSE}
rm(list = ls())
train <- read.csv("phone_train.csv", header=TRUE)
test <- read.csv("phone_validation.csv", header=TRUE)

train$sex <- as.factor(train$sex)
train$payment.method <- as.factor(train$payment.method)
train$activation.zone <- as.factor(train$activation.zone)
train$activation.channel <- as.factor(ifelse(train$activation.channel==5, 1, 0)) #Merge
train$tariff.plan <- as.factor(ifelse(train$tariff.plan <= 6, 1, train$tariff.plan)) #Merge
train$vas1 <- as.factor(train$vas1)
train$vas2 <- as.factor(train$vas2)

test$sex <- as.factor(test$sex)
test$payment.method <- as.factor(test$payment.method)
test$activation.zone <- as.factor(ifelse(test$activation.zone == 0, 1, test$activation.zone))
test$activation.channel <- as.factor(ifelse(test$activation.channel==5, 1, 0)) #Merge
test$tariff.plan <- as.factor(ifelse(test$tariff.plan <= 6, 1, test$tariff.plan)) #Merge
test$vas1 <- as.factor(test$vas1)
test$vas2 <- as.factor(test$vas2)
```

```{r}
plot(train$sex)
plot(train$payment.method)
plot(train$activation.zone)
summary(train$activation.zone)

plot(train$activation.channel)
summary(train$activation.zone)
summary(test$activation.zone)

plot(train[,89:98])

summary(train)
```

```{r}
plotdata <- function (column, title, ylim){
  plot(1, mean(train[,paste("q01", column, sep = "")]), pch=16, col=1, xlab = "Month", ylab = "Value", main = title, xlim = c(1, 9), ylim = ylim)
  for (i in 2:9) {
    points(i, mean(train[, paste("q0", i, column, sep="")]), pch=16, col=i)
  }
}
```



```{r}
plotdata(".out.ch.peak", "mean total monthly number of outgoing calls\nat times when the tariff is highest", c(50, 110))
#plotdata(".out.dur.peak", "mean total monthly duration of outgoing calls\nat times when the tariff is highest", c(5000, 13000))
#plotdata(".out.val.peak", "mean total monthly value of outgoing calls\nat times when the tariff is highest", c(25, 60))

plotdata(".out.ch.offpeak", "mean total monthly number of outgoing calls during off-peak hours", c(0, 10))
#plotdata(".out.dur.offpeak", "mean total monthly duration of outgoing calls during off-peak hours", c(300, 900))
#plotdata(".out.val.offpeak", "mean total monthly value of outgoing calls during off-peak hours", c(0, 4))

plotdata(".in.ch.tot", "mean total monthly number of incoming calls", c(50, 100))
#plotdata(".in.dur.tot", "mean total monthly duration of incoming calls", c(4000, 9000))

plotdata(".ch.sms", "mean total monthly number of SMS sent", c(0, 5))
plotdata(".ch.cc", "mean total monthly number of calls to the Customer Service Centre", c(0, 1))
```

```{r}
for (i in 0:8) {
  index = i * 10 + 9
  corrplot(cor(train[,index:(index+2)]), method = "number")
  #corrplot(cor(train[,(index+3):(index+5)]), method = "number")
  #corrplot(cor(train[,(index+6):(index+7)]), method = "number")
}
```

```{r}
# train$q01.out.dur.peak <- NULL
# train$q01.out.val.peak <- NULL
# train$q02.out.dur.peak <- NULL
# train$q02.out.val.peak <- NULL
# train$q03.out.dur.peak <- NULL
# train$q03.out.val.peak <- NULL
# train$q04.out.dur.peak <- NULL
# train$q04.out.val.peak <- NULL
# train$q05.out.dur.peak <- NULL
# train$q05.out.val.peak <- NULL
# train$q06.out.dur.peak <- NULL
# train$q06.out.val.peak <- NULL
# train$q07.out.dur.peak <- NULL
# train$q07.out.val.peak <- NULL
# train$q08.out.dur.peak <- NULL
# train$q08.out.val.peak <- NULL
# train$q09.out.dur.peak <- NULL
# train$q09.out.val.peak <- NULL
# 
# train$q01.out.dur.offpeak <- NULL
# train$q01.out.val.offpeak <- NULL
# train$q02.out.dur.offpeak <- NULL
# train$q02.out.val.offpeak <- NULL
# train$q03.out.dur.offpeak <- NULL
# train$q03.out.val.offpeak <- NULL
# train$q04.out.dur.offpeak <- NULL
# train$q04.out.val.offpeak <- NULL
# train$q05.out.dur.offpeak <- NULL
# train$q05.out.val.offpeak <- NULL
# train$q06.out.dur.offpeak <- NULL
# train$q06.out.val.offpeak <- NULL
# train$q07.out.dur.offpeak <- NULL
# train$q07.out.val.offpeak <- NULL
# train$q08.out.dur.offpeak <- NULL
# train$q08.out.val.offpeak <- NULL
# train$q09.out.dur.offpeak <- NULL
# train$q09.out.val.offpeak <- NULL
# 
# train$q01.in.dur.tot <- NULL
# train$q02.in.dur.tot <- NULL
# train$q03.in.dur.tot <- NULL
# train$q04.in.dur.tot <- NULL
# train$q05.in.dur.tot <- NULL
# train$q06.in.dur.tot <- NULL
# train$q07.in.dur.tot <- NULL
# train$q08.in.dur.tot <- NULL
# train$q09.in.dur.tot <- NULL

```

```{r}
baseModel <- lm(y ~ 1, data = train)
FwStepSel <- step(baseModel, direction = "forward", scope = list(lower = baseModel, upper = lm(y ~ ., data = train)), trace = 0)

#Based on the previous step results:
fit = lm(formula = FwStepSel$call$formula, data = train)
summary(fit)
#yhat = predict(fit, newdata=test)
```

```{r}
fullModel <- lm(y ~ ., data = train)
BwStepSel <- step(fullModel, direction="backward", trace = 0)


#Based on the previous step results:
fit = lm(formula = BwStepSel$call$formula, data = train)
summary(fit)
yhat = predict(fit, newdata=test)
```

```{r}
library(randomForest)
# Train a Random Forest classification model
rf_model <- randomForest(y ~ ., data = train, mtry = 30, importance = TRUE)
summary(rf_model)
# Extract feature importance
importance(rf_model)
# Visualize feature importance
varImpPlot(rf_model)
yhat <- predict(rf_model, newdata=test)
```

Random forest with mtry search (DO NOT RUN, TOO LONG)

```{r}
# Define a grid of mtry values to search over
# tuneGrid <- expand.grid(mtry = seq(1, ncol(train) - 1, by = 1))

# Set up cross-validation control
#rForestCtrl <- trainControl(method = "cv", number = 5, search = "grid")

# Train the model with the tuning grid
# rForestFit <- train(y ~ ., data = train, method = "rf", preProcess = c("center", "scale"), tuneGrid = tuneGrid, allowParallel = TRUE)

# Get the importance of each feature and plot it
# importance <- importance(rForestFit$finalModel)
# varImpPlot(rForestFit$finalModel)

# Predict the quality of the test set
# yhat = predict(rForestFit, newdata = test)

# Get the final mtry value
# rForestFit$finalModel$mtry

```


Ridge regression
```{r}
library(glmnet)

lambdas <- 10^seq(2, -2, by = -.1)

y_train = train$y
x_train <- train[!names(train) %in% "y"]


train_matrix <- model.matrix(~ . - 1, data = x_train)
test_matrix <- model.matrix(~ . -1, data = test)
str(as.data.frame(train_matrix))
str(as.data.frame(test_matrix))
dim(train_matrix)
dim(test_matrix)
train_matrix <- as.matrix(train_matrix)  # Exclude the response variable
train_y <- train$y
test_matrix <- as.matrix(test_matrix)




ridge_cv <- cv.glmnet(train_matrix, train_y, alpha = 0, lambda = lambdas)
# Best lambda value
best_lambda <- ridge_cv$lambda.min
best_lambda
dim(train_matrix)
dim(test_matrix)
best_ridge <- glmnet(train_matrix, train_y, alpha = 0, lambda = best_lambda)
dim(test)
head(test)
yhat <- predict(best_ridge, s = best_lambda, newx = test_matrix)
yhat
write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE,col.names = FALSE)
```


```{r}
library(parallel) 
no_cores <- 6
library(doParallel)

cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl)
# Define a grid of mtry values to search over
tuneGrid <- expand.grid(mtry = seq(1, ncol(train) - 1, by = 1))

# Set up cross-validation control
#rForestCtrl <- trainControl(method = "cv", number = 5, search = "grid")

# Train the model with the tuning grid
rForestFit <- train(y ~ ., data = train, method = "rf", preProcess = c("center", "scale"), tuneGrid = tuneGrid, allowParallel = TRUE)

# Get the importance of each feature and plot it
importance <- importance(rForestFit$finalModel)
varImpPlot(rForestFit$finalModel)

# Predict the quality of the test set
yhat = predict(rForestFit, newdata = test)

# Get the final mtry value
rForestFit$finalModel$mtry
```

MARS
```{r}
# Train a MARS model with bagEarthGCV using Caret
# Grid search for the best degree
mars_model <- caret::train(y ~ ., data = train, method = "gcvEarth", trControl = trainControl(method = "cv", number = 10), tuneGrid = data.frame(degree = 1:3))
summary(mars_model)
# Visualize the MARS model
plot(mars_model)

# Predict the quality of the test set
yhat = predict(mars_model, newdata = test)
write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE,col.names = FALSE)

```
```{r}
#fit <- lm(y ~ . , data=train)
#summary(fit)
#yhat <- predict(fit, newdata=test)
write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE,col.names = FALSE)
```

--------------------------------------------------------------------------------------
NEW WORK BY DIVING FOR CALLS WITH ZERO MINUTES AND POSITIVE ONES

We turn our attention to the training dataset. The initial graphic descriptive analysis reveals that the response variable is greatly skewed around zero. Specifically, there are many customers in the training data who did not had outgoing calls. This characteristic of the data causes some complications when automatically applying regression models. The response variable cannot be considered continuous due to its mixed variable traits:
- a continuous element is present for some observations, which corresponds to the minutes of outgoing calls made by the customers.
- a discrete binary component is found with the group of customers that didn't make any calls within that particular month.

The following histograms show the distribution of the minutes of outgoing calls for all customers and for customers with more than 0 minutes of outgoing calls.

```{r}
# Histogram of minutes of outgoing calls
hist(train$y, breaks = 10000, main = "Histogram of minutes of outgoing calls", xlab = "Minutes of outgoing calls", xlim = c(0, 1000))

# Histogram of minutes of outgoing calls for customers with more than 0 minutes
hist(train$y[train$y > 0], breaks = 10000, main = "Histogram of minutes of outgoing calls for customers with more than 0 minutes", xlab = "Minutes of outgoing calls", xlim = c(0, 1000))
```

We now build a logistic regression model to predict whether a customer has a positive number of minutes of outgoing calls. With this model, we aim to predict the binary response variable y_binary, which is 1 if the customer has more than 0 minutes of outgoing calls and 0 otherwise. We split the training data into training and testing sets, train the logistic regression model on the training set, and test the model on the testing set. We calculate the accuracy of the model on the testing set. If the models predicts that a user has more than 0 minutes of outgoing calls, we will predict the mean of the minutes of outgoing calls for that user by using a regression model.

```{r}
# Create a new binary response variable. 1 if the customer has more than 0 minutes of outgoing calls, 0 otherwise
train$y_binary <- factor(ifelse(train$y > 0, 1, 0), levels = c(0, 1))

# Split the data into training and testing sets
train_index <- createDataPartition(train$y_binary, p = 0.7, list = FALSE)
glm.train <- train[train_index, ]
glm.test <- train[-train_index, ]

# Remove y from the training and testing sets to avoid using it as a predictor
glm.train$y <- NULL
glm.test$y <- NULL

# Train a logistic regression model
# Since the binary response depends only on the actual call traffic, we remove the other variables that are not related to the call traffic 
#logistic_model <- glm(y_binary ~ . -tariff.plan -vas1 -vas2 -sex -age -activation.zone -activation.channel -payment.method, data = glm.train, family = "binomial")

logistic_model <- glm(y_binary ~ . -tariff.plan -vas1 -vas2 -sex -age -activation.zone -activation.channel -payment.method, data = glm.train, family = "binomial")

# Test the model on the testing set
glm.yhat <- predict(logistic_model, newdata = glm.test, type = "response")
glm.yhat <- factor(ifelse(glm.yhat > 0.5, 1, 0), levels = c(0, 1))

# Plot the number of positive duration both in general and in the training and test sets
par(mfrow=c(1,2))
barplot(table(train$y_binary), main = "TRAIN: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")
barplot(table(glm.yhat), main = "PREDICTED: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")

# Calculate the accuracy of the model with confusion matrix
confusion_matrix <- confusionMatrix(glm.yhat, glm.test$y_binary)
confusion_matrix$overall["Accuracy"]

```
Same thing but with decision tree. Accuracy is improved.

```{r}
treeDf <- train
# Create a new binary response variable. 1 if the customer has more than 0 minutes of outgoing calls, 0 otherwise
treeDf$y_binary <- factor(ifelse(train$y > 0, 1, 0), levels = c(0, 1))
# Remove y to avoid using it as a predictor
treeDf$y <- NULL

# Split the data into training and testing sets
train_index <- createDataPartition(treeDf$y_binary, p = 0.7, list = FALSE)
treeDf.train <- treeDf[train_index, ]
treeDf.test <- treeDf[-train_index, ]

# Fit a regression tree on all the features with a very small pruning level
rTreeFit <- tree(y_binary ~ ., data = treeDf.train, control = tree.control(nobs = nrow(treeDf), mindev = 0.002))

# Plot the tree
summary(rTreeFit)
plot(rTreeFit)
text(rTreeFit, pretty = 0)

# Cross-validation to find the optimal pruning level using the deviance
rTreeCv <- cv.tree(rTreeFit, method = "deviance")
plot(rTreeCv$size, rTreeCv$dev, type = "b", xlab = "Tree Size", ylab = "Deviance", main = "Pruning VS Deviance Plot")

# Prune the tree to the optimal level
prunedTree <- prune.tree(rTreeFit, best = 6)

# Plot the final pruned tree
summary(prunedTree)
plot(prunedTree)
text(prunedTree, pretty = 1)

# Predictions on the test set
treeDf.yhat <- predict(prunedTree, newdata = treeDf.test, type = "class")

# Plot the number of positive duration both in general and in the training and test sets
par(mfrow=c(1,2))
barplot(table(treeDf.train$y_binary), main = "TRAIN: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")
barplot(table(treeDf.yhat ), main = "PREDICTED: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")

# Calculate the accuracy of the model with confusion matrix
confusion_matrix <- confusionMatrix(treeDf.yhat, treeDf.test$y_binary)
confusion_matrix$overall["Accuracy"]

# Fit the tree on the whole dataset
rTreeFit <- tree(y_binary ~ ., data = treeDf, control = tree.control(nobs = nrow(train), mindev = 0.001))
prunedTree <- prune.tree(rTreeFit, best = 6)
plot(prunedTree)
text(prunedTree, pretty = 1)
```

Same thing but with random forest. Accuracy is improved.
```{r}
rfDf <- train
# Create a new binary response variable. 1 if the customer has more than 0 minutes of outgoing calls, 0 otherwise
rfDf$y_binary <- factor(ifelse(train$y > 0, 1, 0), levels = c(0, 1))
# Remove y to avoid using it as a predictor
rfDf$y <- NULL

# Split the data into training and testing sets
train_index <- createDataPartition(rfDf$y_binary, p = 0.7, list = FALSE)
rfDf.train <- rfDf[train_index, ]
rfDf.test <- rfDf[-train_index, ]

# Fit a random forest model
rfFit <- caret::train(y_binary ~ ., data = rfDf.train, method = "rf", trControl = trainControl(method = "cv", number = 5), preProcess = c("center", "scale"), mttry = 60)

# Predictions on the test set
rfDf.yhat <- predict(rfFit, newdata = rfDf.test)

# Plot the number of positive duration both in general and in the training and test sets
par(mfrow=c(1,2))
barplot(table(rfDf.train$y_binary), main = "TRAIN: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")
barplot(table(rfDf.yhat), main = "PREDICTED: Number of call with positive and zero dur.", xlab = "Positive duration", ylab = "Frequency")

# Calculate the accuracy of the model with confusion matrix
confusion_matrix <- confusionMatrix(rfDf.yhat, rfDf.test$y_binary)
confusion_matrix$overall["Accuracy"]

```



Train a model on samples with the response different from 0 (MARS)
```{r}
# Create a new data frame with all 
marsDf <- train[train$y > 0, ]
marsDf$y_binary <- NULL

# Train a MARS model with bagEarthGCV using Caret
# Grid search for the best degree
mars_model <- caret::train(y ~ ., data = marsDf, method = "gcvEarth", trControl = trainControl(method = "cv", number = 10), tuneGrid = data.frame(degree = 1:10))

# Use FwStepSel$call$formula
#mars_model <- caret::train(FwStepSel$call$formula, data = marsDf, method = "gcvEarth", trControl = trainControl(method = "cv", number = 10), tuneGrid = data.frame(degree = 1:5))

summary(mars_model)
# Visualize the MARS model
plot(mars_model)

# Get the best model
best_mars_model <- mars_model$finalModel
```

Train a model on samples with the response different from 0 (LINEAR)
```{r}
# Create a new data frame with all 
lmDf <- train[train$y > 0, ]
lmDf$y_binary <- NULL

# Train a linear model with with caret
lm_model <- caret::train(y ~ ., data = lmDf, method = "lm", trControl = trainControl(method = "cv", number = 10))
summary(lm_model)
```


Train a model on samples with the response different from 0
```{r}
# Custom model to predict the minutes of outgoing calls for customers
# First, apply the tree model to get the outcome. If the outcome is different from zero, apply the regression model to get the final response

combinedModel <- function(row) {
  y = predict(rfFit, newdata = row, type = "raw")
  # If the prediction outcome is no traffic, return 0
  if(y == 0)
    as.numeric(0)
  # else return predicted value using regression
  else
    predict(mars_model, newdata = row)
}

yhat <- rep(0, nrow(test))
# Predict all values of test set
for(i in 1:nrow(test)) {
  testRow <- test[i, ]
  yhat[i] <- combinedModel(testRow)
}


write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE,col.names = FALSE)


```








