---
title: "PhoneUsers"
author: "Group D"
date: "2024-04-10"
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: sentence
---

```{r include=FALSE}
# Clear workspace, install library & import data
library("caret")
library("Metrics")
library("corrplot")
library("earth")
library("tree")
library("readr")
library("dplyr")
library("GGally")

set.seed(1234)
```

```{r include=FALSE}
rm(list = ls())
train <- read.csv("phone_train.csv", header=TRUE)
test <- read.csv("phone_validation.csv", header=TRUE)

# Ensure that the features are in the right format
train$sex <- as.factor(train$sex)
train$payment.method <- as.factor(train$payment.method)
train$activation.zone <- as.factor(train$activation.zone)
train$activation.channel <- as.factor(ifelse(train$activation.channel==5, 1, 0)) #Merge
train$tariff.plan <- as.factor(ifelse(train$tariff.plan <= 6, 1, train$tariff.plan)) #Merge
train$vas1 <- as.factor(train$vas1)
train$vas2 <- as.factor(train$vas2)

test$sex <- as.factor(test$sex)
test$payment.method <- as.factor(test$payment.method)
test$activation.zone <- as.factor(ifelse(test$activation.zone == 0, 1, test$activation.zone))
test$activation.channel <- as.factor(ifelse(test$activation.channel==5, 1, 0)) #Merge
test$tariff.plan <- as.factor(ifelse(test$tariff.plan <= 6, 1, test$tariff.plan)) #Merge
test$vas1 <- as.factor(test$vas1)
test$vas2 <- as.factor(test$vas2)

# Save feature names in vectors to be used later
q09 <- c("q09.in.dur.tot", "q09.in.ch.tot", "q09.out.val.peak", "q09.out.dur.peak", "q09.out.ch.peak")
q08 <- c("q08.in.dur.tot", "q08.in.ch.tot", "q08.out.val.peak", "q08.out.dur.peak", "q08.out.ch.peak")
q07 <- c("q07.in.dur.tot", "q07.in.ch.tot", "q07.out.val.peak", "q07.out.dur.peak", "q07.out.ch.peak")
other <- c("age", "sex", "payment.method", "activation.zone", "activation.channel", "tariff.plan", "vas1", "vas2")
```

# Initial analysis of the dataset

The dataset consists of 10,000 entries and 99 variables, capturing various characteristics and behaviors of customers over 9 months.
Hereâ€™s a summary of the key points derived from the initial exploration of the dataset:

## Data Quality:

**Missing Values:** There are no missing values in the dataset.

**Outliers:** The dataset contains some outliers, particularly in traffic-related variables, which could impact model performance if not addressed.

**Categorical Variables:** Many variables are categorical and may require encoding.They have been converted to Factors for reassurance after loading the dataset.

## Customer Demographics and Characteristics (Categorical Variables):

**tariff.plan:** Categorical variable with 5 levels indicating different customer tariff plans.

**payment.method:** Categorical variable with 3 levels (Postal Order, Credit Card, Direct Debit).

**sex:** Categorical variable with 3 levels (Male, Female, Company).

**age:** Continuous variable representing the age of the customers.

**activation.zone:** Categorical variable with 4 levels representing geographical zones.

**activation.channel:** Categorical variable with 8 levels indicating the channel through which customers activated their services.

**vas1**, **vas2:** Binary variables indicating the presence of value-added services.

## Traffic Data Over 9 Months:

For each month (q01 to q9), the dataset includes variables for:

**qnn.out.ch.peak:** Total monthly number of outgoing calls at peak tariff times.

**qnn.out.dur.peak:** Duration of total monthly outgoing calls at peak tariff times.

**qnn.out.val.peak:** Total monthly outgoing call value at peak tariff times.

**qnn.out.ch.offpeak:** Total monthly number of outgoing calls at off-peak tariff times.

**qnn.out.dur.offpeak:** Duration of total monthly outgoing calls at off-peak tariff times.

**qnn.out.val.offpeak:** Total monthly outgoing call value at off-peak tariff times.

**qnn.in.ch.tot:** Total monthly number of incoming calls.

**qnn.in.dur.tot:** Duration of total monthly incoming calls.

**qnn.ch.sms:** Total monthly number of SMS sent.

**qnn.ch.cc:** Number of monthly calls to customer services.

## Initial Observations:

**Age Distribution:** The age of customers varies widely, with a mean of approximately 38.1 years and a standard deviation of 12.6 years.

**Traffic Patterns:** There are significant variances in call duration, call value, and number of calls both at peak and off-peak times.
The data shows some customers have zero traffic in certain months, indicating periods of inactivity.

**SMS and Customer Service Calls:** The number of SMS sent and calls to customer service also show wide variability, with some customers having very high engagement in these activities.

**Deactivation Rates:** The binary deactivation status provides a critical outcome variable for predictive modeling, useful for understanding customer churn.

### Principal Component Analysis
Here, PCA is used to identify the most important features in the dataset in order to better understand the data. By default, the prcomp() function, with the option "scale = true", centers and scales the data, so we don't need to do it manually.

This biplot tells us that the variables "*.in.dur.tot", "*.in.ch.tot", "*.out.val.peak", "*.out.dur.peak", "*.out.ch.peak" of the different months are highly correlated, which is expected since they are all related to the same type of activity (calls). Instead, the variables of sex, payment method, activation zone, activation channel and tariff plan are not correlated with the other variables, which is also expected since they aren't strictly related to the call duration. Indeed, VAS1 and VAS2, which refers to the presence of value-added services, show some correlation with the call duration, meaning that those who have these services tend to make more calls.

The biplot clearly explain this by placing the variables related to the call duration along the first principal component and the categorical variables along the second principal component, orthogonal to the first one.

```{r}
library("ggbiplot")
# Principal Component Analysis
pcaTrain <- train[, c(q09, q08, other, "y")]
pcaTrain$sex <- as.numeric(pcaTrain$sex)
pcaTrain$payment.method <- as.numeric(pcaTrain$payment.method)
pcaTrain$activation.zone <- as.numeric(pcaTrain$activation.zone)
pcaTrain$activation.channel <- as.numeric(pcaTrain$activation.channel)
pcaTrain$tariff.plan <- as.numeric(pcaTrain$tariff.plan)
pcaTrain$vas1 <- as.numeric(pcaTrain$vas1)
pcaTrain$vas2 <- as.numeric(pcaTrain$vas2)

pca <- prcomp(pcaTrain[,1:19], scale = TRUE)

# Biplot with GGplot2
ggbiplot::ggbiplot(pca, obs.scale = 1, var.scale = 1, groups = pcaTrain$y, circle = TRUE, varname.size = 3, varname.adjust = 3, varname.color = "orange", varname.face = "bold") +
  theme_minimal() +
  theme(legend.title = element_text(size = 10), legend.text = element_text(size = 10)) +
  ggtitle("PCA Biplot")
```
# Feature selection

The first thought that came to our heads is that nine months is too much data. Also data about SMS and customer service calls seemed redundant. To prove that, let's check the correlation matrices with the most promising (logically) features: the duration of calls during peak time and off peak:

```{r}
matching_cols <- grep("^q0[1-9]\\.out\\.dur\\.peak$", names(train), value = TRUE)
train_out_peak <- train[, matching_cols]
corrplot(cor(train_out_peak))
matching_cols <- grep("^q0[1-9]\\.out\\.dur\\.offpeak$", names(train), value = TRUE)
train_out_offpeak <- train[, matching_cols]
corrplot(cor(train_out_peak))
```

As we can see, the correlation between different months is very high, because they are telling the same story. This means that we can use only the last two months' data to predict the duration of calls in the tenth month. This will reduce the number of features and make the model more interpretable.


# Two-step model: classification and regression

By analyzing the training data set, it is possible to see that many of the values of total monthly duration (in seconds) of outgoing calls in the response are zero.
This means that many users did not make any outgoing calls in the predicted month and so it my be unnecessary to predict the duration of calls for these users.
Therefore, we propose a two-step model to predict the response.
The first step is a classification model that predicts if the user has a positive duration of calls and the second step is a regression model that predicts the duration of calls for the users that have a positive duration of calls.

In the following histogram, we can see that the distribution of the response variable is highly skewed to the left This is because many users did not make any outgoing calls in the tenth month.
It is so skewed that it is hard to see the real distribution of call duration.
In the second histogram, by removing the users with zero duration of calls, we can see the real distribution of call duration.
The plots are limited to 1000 minutes of calls for better visualization.

By having two models we can have more control over the learning process.
The features that influence whether a call has zero duration may differ from those that influence the length of a call.
For instance, the age feature might be more relevant to zero-duration calls, while the tariff plan might be more relevant for predicting call duration (young people might be less likely to make calls because of messaging apps, while older people might make longer calls).
Separate models can provide clearer insights into the different factors affecting zero-duration calls and actual call duration.
This can be particularly useful for business decisions, such as addressing the causes of zero-duration calls or improving user experience to increase call duration.

```{r}
# Histogram of minutes of outgoing calls
hist(train$y, breaks = 50000, main = "Histogram of minutes of outgoing calls", xlab = "Minutes of outgoing calls", xlim = c(0, 1000), col = "#2980b9")

# Histogram of minutes of outgoing calls for customers with more than 0 minutes
hist(train$y[train$y > 0], breaks = 50000, main = "Histogram of minutes of outgoing calls for customers with more than 0 minutes", xlab = "Minutes of outgoing calls", xlim = c(0, 1000), col = "#2980b9")

```

Just for the sake of having a complete view of the data, we also plot the ratio of users with positive duration of calls to the total number of users.

```{r}
# Plot the number of users with positive duration of calls and number of users with zero duration of calls
barplot(table(ifelse(train$y > 0, "Positive duration", "Zero duration")), main = "Number of samples with positive duration vs zero duration of calls", xlab = "Duration of calls", ylab = "Number of samples", col = c("#2980b9", "#e74c3c"))
```

# Classification

As we have already seen above, we decided to split the problem in a 2-step approach, in order to have more control on the learning process.
The first step leads to a binary classification approach, where an object is classified as 1 if it did call for at least one minute, 0 otherwise.
To do this, we decided to use an ensamble method, taking advantage of the low variance that these models exploit.
This is achieved because of the fact that we are averaging a set of observations, with a Random Forest.
With Random Forests we are building a set of decision trees on bootstrapped training samples, but each time a random sample of predictors is considered from the full set of predictors.
In this way, we are building de-correlated trees that use different features, making the average of the resulting trees less variable, hence the final model is more reliable.
Random Forests can be really helpful with high dimensional data, and with our 19-dimensional dataset this model gives good results, for what concerns the metrics that we take into account.

An important evaluation is given by sensitivity and specificity, since the distribution of the positive samples (who's made at least one call) is higher than negative samples, ad thus accuracy measure can be biased.
Another important aspect that specificity is taking into account is the proportion of actual negatives.
In fact for our purpose the identification of actual positive samples and actual negative samples can lead to accurate evaluations and better results.
0-minute-calls classified as positive-duration-calls are really different than positive-duration-calls classified as 0-minute-calls, since the final model can predict a duration for false positives near zero, while in the other case for false negatives we are simply deleting observations from our dataset.

We obtained the value for the number of predictors using Grid-Search Cross-Validation; the final model take into account 4 predictors, as we expected since the features of the dataset created from the last two months are correlated each other.
Sensitivity and Specificity have both values above 80% while for Precision the value is above 90%.
We can argue that this binary classification model is working good as expected, so we can move fro the last part of our path - the regression model.


```{r}
rfDf <- train
# Create a new binary response variable. 1 if the customer has more than 0 minutes of outgoing calls, 0 otherwise
rfDf$y_binary <- factor(ifelse(train$y > 0, 1, 0), levels = c(0, 1))
# Remove y to avoid using it as a predictor
rfDf$y <- NULL

# Create a new data frame with the last three months of data (feature starting with q09, q08, q07)

rfDfLastTwoMonths <- rfDf[, c(q09, q08, other, "y_binary")]

# Fit a random forest model
tuneGrid <- expand.grid(.mtry = 2:18)
rfFit <- caret::train(y_binary ~ .,
                      data = rfDfLastTwoMonths,
                      method = "rf",
                      trControl = trainControl(method = "cv", number = 10),
                      tuneGrid = tuneGrid,
                      #mttry = 4,
                      preProcess = c("center", "scale"))

# Confusion matrix
confusionMatrix <- caret::confusionMatrix(rfFit)
confusionMatrix

#best_mtry <- rfFit$bestTune$mtry
#cat("Best mtry: ", best_mtry, "\n")

rfFit.tp <- confusionMatrix$table[2, 2] # True positive: the number of positive instances correctly classified
rfFit.tn <- confusionMatrix$table[1, 1] # True negative: the number of negative instances correctly classified
rfFit.fp <- confusionMatrix$table[1, 2] # False positive: the number of negative instances incorrectly classified as positive
rfFit.fn <- confusionMatrix$table[2, 1] # False negative: the number of positive instances incorrectly classified as negative

# Accuracy: the proportion of true results (both true positives and true negatives) among the total number of cases examined.
rfFit.accuracy <- (rfFit.tp + rfFit.tn) / (rfFit.tp + rfFit.tn + rfFit.fp + rfFit.fn)
cat("Accuracy: ", rfFit.accuracy, "\n")

# Precision: the proportion of true positive results among the positive predictions.
rfFit.precision <- rfFit.tp / (rfFit.tp + rfFit.fp)
cat("Precision: ", rfFit.precision, "\n")

# Recall (Sensitivity): the proportion of actual positives correctly identified by the classifier.
rfFit.recall <- rfFit.tp / (rfFit.tp + rfFit.fn)
cat("Recall: ", rfFit.recall, "\n")

# Specificity: the proportion of actual negatives correctly identified by the classifier.
rfFit.specificity <- rfFit.tn / (rfFit.tn + rfFit.fp)
cat("Specificity: ", rfFit.specificity, "\n")

```

```{r}
posDf <- train[train$y > 0, ]
posDf$y_binary <- NULL

posDf$tariff.plan <- as.numeric(posDf$tariff.plan)
posDf$activation.channel <- as.numeric(posDf$activation.channel)
posDf$activation.zone <- as.numeric(posDf$activation.zone)
posDf$payment.method <- as.numeric(posDf$payment.method)
posDf$sex <- as.numeric(posDf$sex)
posDf$vas1 <- as.numeric(posDf$vas1)
posDf$vas2 <- as.numeric(posDf$vas2)

correlationMatrix <- cor(posDf)
correlationMatrixY <- correlationMatrix["y", ]
correlationMatrixY <- correlationMatrixY[order(abs(correlationMatrixY), decreasing = TRUE)]
barplot(correlationMatrixY, las = 2, cex.names = 0.7, main = "Correlation of features with y")
```

# Regression

For the regression we utilized a Multivariate Adaptive Regression Splines (MARS) model to predict the duration of outgoing calls using our dataset.
Initially, we prepared the training dataset by removing the binary response variable, y_binary.
Data from the last two months (q09 and q08), along with other relevant features and the target variable y, were combined into a new dataframe, marsDfLastTwoMonths.

We then trained the MARS model using the train function from the caret package, with the target variable transformed using log(y+1) to handle skewness.
The "gcvEarth" method, which stands for Generalized Cross-Validation for Earth models (MARS), was employed.
Subsequently, we conducted a grid search over the degrees of interaction (from 1 to 5) to find the optimal model, where interaction degree refers to the complexity of the interactions between variables considered by the model.

The summary of the trained MARS model was printed, and the model was visualized using a plot to show the relationship between the interaction degree and the Root Mean Square Error (RMSE) during cross-validation.
The console output displayed the coefficients of the MARS model, indicating which terms and interactions between terms were included in the final model.
Our model selected 33 out of 43 possible terms and 10 out of 23 predictors.
Significant predictors included "q09.out.ch.peak," "tariff.plan8," "tariff.plan7," "q09.out.val.peak," and "age."

The goodness of fit was indicated by the Generalized Cross-Validation (GCV) score, Residual Sum of Squares (RSS), and R-squared values, suggesting a moderate fit (GCV: 4.678, RSS: 46027.67, GRSq: 0.567, RSq: 0.574).
The plot illustrated the RMSE against the degrees of interaction, indicating that the RMSE decreases significantly up to an interaction degree of 3, after which it stabilizes.
This suggests that adding more complexity beyond three-way interactions does not substantially improve model performance.

```{r}
# Create a new data frame with all 
#marsDf <- train[train$y > 0, ]
marsDf <- train
marsDf$y_binary <- NULL

marsDfLastTwoMonths <- marsDf[, c(q09, q08, other, "y")]

# Train a MARS model with bagEarthGCV using Caret
# Grid search for the best degree
mars_model <- caret::train(log(y+1) ~ ., data = marsDfLastTwoMonths, method = "gcvEarth", trControl = trainControl(method = "cv", number = 10), tuneGrid = data.frame(degree = 1:5), preProcess = c("center", "scale"))

summary(mars_model)
# Visualize the MARS model
plot(mars_model)

# Get the best model
best_mars_model <- mars_model$finalModel
```

# Model combination

The final model combines the classification model and the regression model.
We have created a function that accepts the test set, the classification model, and the regression model as input and returns the predicted values.
The function first predicts the binary outcome using the random forest model.
If the predicted outcome is non-zero, the function applies the regression model to predict the duration of calls.
To make this function faster, we first predict the binary outcome for the entire test set and then apply the regression model only to the rows with positive outcomes.
The other alternative was to apply the two models to each row of the test set, but this was much slower due to the way predict() works in R.

The threshold for the binary outcome is set to 0.25.
This threshold was chosen based on the confusion matrix of the random forest model and it is set this low because it is better to have false positives that can still be predicted by the regression model than false negatives that will not be predicted by the regression model and will be set to zero.

After returning the predicted values, the script writes the results to a file.

```{r}
combinedModel <- function(test, classModel, regModel) {
  # Predict outcomes using the random forest model for the entire test set
  y_rf = predict(classModel, newdata = test, type = "prob")
  tresh = 0.25 
  y_rf = ifelse(y_rf[, 2] > tresh, 1, 0)

  # Initialize yhat with zeros
  yhat = rep(0, nrow(test))
  
  # Identify the rows where the prediction outcome is non-zero
  non_zero_indices = which(y_rf != 0)
  
  # Apply the regression model only to the rows with non-zero outcomes
  if(length(non_zero_indices) > 0) {
    yhat[non_zero_indices] = exp(predict(regModel, newdata = test[non_zero_indices, ]))-1
  }
  
  return(yhat)
}

# Predict all values of test set
yhat <- combinedModel(test, rfFit, mars_model)

# Write the results to a file
write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE, col.names = FALSE)
```

# Conclusions
The data set and the problem turned out to be pretty complex and required a deep analysis to understand the meaning of the features, what features to keep, and what model to use.
We find out that:
1) Many users did not make any outgoing calls in the tenth month.
2) The data from different months is was correlated.
3) Many features from months data was redundant and can be removed.

After deciding to use only the data from the last two months, we decided to use a two-step model: classification and regression. The classification model predicts if the user has a positive duration of calls and the regression model predicts the duration of calls for the users that have a positive duration of calls.

It is possible that more advanced and complex models could have led to better results, but our goal was to apply the content of the course and to provide a simple and interpretable model that we could fully understand and explain. Furthermore, applying a two-step model was a new approach to us that we wanted to explore because it allowed us to have more control over the final model, to applying models for both classification and regression and to avoid choosing a model only based on the final score and without really understanding it. In the end we are happy both with the results and with the knowledge we gained from this project.







