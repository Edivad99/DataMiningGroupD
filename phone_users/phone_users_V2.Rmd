---
title: "PhoneUsers"
author: "Group D"
date: "2024-04-10"
output: html_document
---

```{r include=FALSE}
# Clear workspace, install library & import data
library("caret")
library("Metrics")
library("corrplot")
library("earth")
library("tree")

set.seed(1234)
```

```{r include=FALSE}
rm(list = ls())
train <- read.csv("phone_train.csv", header=TRUE)
test <- read.csv("phone_validation.csv", header=TRUE)

# Ensure that the features are in the right format
train$sex <- as.factor(train$sex)
train$payment.method <- as.factor(train$payment.method)
train$activation.zone <- as.factor(train$activation.zone)
train$activation.channel <- as.factor(ifelse(train$activation.channel==5, 1, 0)) #Merge
train$tariff.plan <- as.factor(ifelse(train$tariff.plan <= 6, 1, train$tariff.plan)) #Merge
train$vas1 <- as.factor(train$vas1)
train$vas2 <- as.factor(train$vas2)

test$sex <- as.factor(test$sex)
test$payment.method <- as.factor(test$payment.method)
test$activation.zone <- as.factor(ifelse(test$activation.zone == 0, 1, test$activation.zone))
test$activation.channel <- as.factor(ifelse(test$activation.channel==5, 1, 0)) #Merge
test$tariff.plan <- as.factor(ifelse(test$tariff.plan <= 6, 1, test$tariff.plan)) #Merge
test$vas1 <- as.factor(test$vas1)
test$vas2 <- as.factor(test$vas2)

# Save feature names in vectors to be used later
q09 <- c("q09.in.dur.tot", "q09.in.ch.tot", "q09.out.val.peak", "q09.out.dur.peak", "q09.out.ch.peak")
q08 <- c("q08.in.dur.tot", "q08.in.ch.tot", "q08.out.val.peak", "q08.out.dur.peak", "q08.out.ch.peak")
q07 <- c("q07.in.dur.tot", "q07.in.ch.tot", "q07.out.val.peak", "q07.out.dur.peak", "q07.out.ch.peak")
other <- c("age", "sex", "payment.method", "activation.zone", "activation.channel", "tariff.plan", "vas1", "vas2")
```

# Initial analysis of the dataset
## TODO
# Feature selection
## Todo: using only the data from the last two months

# Two-step model: classification and regression
By analyzing the training data set, it is possible to see that many of the values of total monthly duration (in seconds) of outgoing calls in the tenth month are zero. This means that many users did not make any outgoing calls in the tenth month and so it my be unnecessary to predict the duration of calls for these users. Therefore, we propose a two-step model to predict the response. The first step is a classification model that predicts if the user has a positive duration of calls and the second step is a regression model that predicts the duration of calls for the users that have a positive duration of calls.

In the following histogram, we can see that the distribution of the response variable is highly skewed to the left This is because many users did not make any outgoing calls in the tenth month. It is so skewed that it is hard to see the real distribution of call duration. In the second histogram, by removing the users with zero duration of calls, we can see the real distribution of call duration. The plots are limited to 1000 minutes of calls for better visualization.

By having two models we can have more control over the learning process. The features that influence whether a call has zero duration may differ from those that influence the length of a call. For instance, the age feature might be more relevant to zero-duration calls, while the tariff plan might be more relevant for predicting call duration (young people might be less likely to make calls because of messaging apps, while older people might make longer calls). Separate models can provide clearer insights into the different factors affecting zero-duration calls and actual call duration. This can be particularly useful for business decisions, such as addressing the causes of zero-duration calls or improving user experience to increase call duration.

```{r}
# Histogram of minutes of outgoing calls
hist(train$y, breaks = 50000, main = "Histogram of minutes of outgoing calls", xlab = "Minutes of outgoing calls", xlim = c(0, 1000), col = "#2980b9")

# Histogram of minutes of outgoing calls for customers with more than 0 minutes
hist(train$y[train$y > 0], breaks = 50000, main = "Histogram of minutes of outgoing calls for customers with more than 0 minutes", xlab = "Minutes of outgoing calls", xlim = c(0, 1000), col = "#2980b9")

```
Just for the sake of having a complete view of the data, we also plot the ratio of users with positive duration of calls to the total number of users.

```{r}
# Plot the number of users with positive duration of calls and number of users with zero duration of calls
barplot(table(ifelse(train$y > 0, "Positive duration", "Zero duration")), main = "Number of samples with positive duration vs zero duration of calls", xlab = "Duration of calls", ylab = "Number of samples", col = c("#2980b9", "#e74c3c"))
```


# Classification
# Todo: how to use rf for classification + confusion matrix results
```{r}
rfDf <- train
# Create a new binary response variable. 1 if the customer has more than 0 minutes of outgoing calls, 0 otherwise
rfDf$y_binary <- factor(ifelse(train$y > 0, 1, 0), levels = c(0, 1))
# Remove y to avoid using it as a predictor
rfDf$y <- NULL

# Create a new data frame with the last three months of data (feature starting with q09, q08, q07)

rfDfLastTwoMonths <- rfDf[, c(q09, q08, other, "y_binary")]

# Fit a random forest model
rfFit <- caret::train(y_binary ~ ., data = rfDfLastTwoMonths, method = "rf", trControl = trainControl(method = "cv", number = 10), preProcess = c("center", "scale"), mttry = 30)

# Confusion matrix
confusionMatrix <- caret::confusionMatrix(rfFit)
confusionMatrix

rfFit.tp <- confusionMatrix$table[2, 2] # True positive: the number of positive instances correctly classified
rfFit.tn <- confusionMatrix$table[1, 1] # True negative: the number of negative instances correctly classified
rfFit.fp <- confusionMatrix$table[1, 2] # False positive: the number of negative instances incorrectly classified as positive
rfFit.fn <- confusionMatrix$table[2, 1] # False negative: the number of positive instances incorrectly classified as negative

# Accuracy: the proportion of true results (both true positives and true negatives) among the total number of cases examined.
rfFit.accuracy <- (rfFit.tp + rfFit.tn) / (rfFit.tp + rfFit.tn + rfFit.fp + rfFit.fn)
cat("Accuracy: ", rfFit.accuracy, "\n")

# Precision: the proportion of true positive results among the positive predictions.
rfFit.precision <- rfFit.tp / (rfFit.tp + rfFit.fp)
rfFit.precision

# Recall (Sensitivity): the proportion of actual positives correctly identified by the classifier.
rfFit.recall <- rfFit.tp / (rfFit.tp + rfFit.fn)
cat("Recall: ", rfFit.recall, "\n")

# Specificity: the proportion of actual negatives correctly identified by the classifier.
rfFit.specificity <- rfFit.tn / (rfFit.tn + rfFit.fp)
cat("Specificity: ", rfFit.specificity, "\n")

```


```{r}
posDf <- train[train$y > 0, ]
posDf$y_binary <- NULL

posDf$tariff.plan <- as.numeric(posDf$tariff.plan)
posDf$activation.channel <- as.numeric(posDf$activation.channel)
posDf$activation.zone <- as.numeric(posDf$activation.zone)
posDf$payment.method <- as.numeric(posDf$payment.method)
posDf$sex <- as.numeric(posDf$sex)
posDf$vas1 <- as.numeric(posDf$vas1)
posDf$vas2 <- as.numeric(posDf$vas2)

correlationMatrix <- cor(posDf)
correlationMatrixY <- correlationMatrix["y", ]
correlationMatrixY <- correlationMatrixY[order(abs(correlationMatrixY), decreasing = TRUE)]
barplot(correlationMatrixY, las = 2, cex.names = 0.7, main = "Correlation of features with y")
```


# Regression
For the regression we utilized a Multivariate Adaptive Regression Splines (MARS) model to predict the duration of outgoing calls using our dataset. Initially, we prepared the training dataset by removing the binary response variable, y_binary. Data from the last two months (q09 and q08), along with other relevant features and the target variable y, were combined into a new dataframe, marsDfLastTwoMonths.

We then trained the MARS model using the train function from the caret package, with the target variable transformed using log(y+1) to handle skewness. The "gcvEarth" method, which stands for Generalized Cross-Validation for Earth models (MARS), was employed. Subsequently, we conducted a grid search over the degrees of interaction (from 1 to 5) to find the optimal model, where interaction degree refers to the complexity of the interactions between variables considered by the model.

The summary of the trained MARS model was printed, and the model was visualized using a plot to show the relationship between the interaction degree and the Root Mean Square Error (RMSE) during cross-validation. The console output displayed the coefficients of the MARS model, indicating which terms and interactions between terms were included in the final model. Our model selected 33 out of 43 possible terms and 10 out of 23 predictors. Significant predictors included "q09.out.ch.peak," "tariff.plan8," "tariff.plan7," "q09.out.val.peak," and "age."

The goodness of fit was indicated by the Generalized Cross-Validation (GCV) score, Residual Sum of Squares (RSS), and R-squared values, suggesting a moderate fit (GCV: 4.678, RSS: 46027.67, GRSq: 0.567, RSq: 0.574). The plot illustrated the RMSE against the degrees of interaction, indicating that the RMSE decreases significantly up to an interaction degree of 3, after which it stabilizes. This suggests that adding more complexity beyond three-way interactions does not substantially improve model performance.

```{r}
# Create a new data frame with all 
#marsDf <- train[train$y > 0, ]
marsDf <- train
marsDf$y_binary <- NULL

marsDfLastTwoMonths <- marsDf[, c(q09, q08, other, "y")]

# Train a MARS model with bagEarthGCV using Caret
# Grid search for the best degree
mars_model <- caret::train(log(y+1) ~ ., data = marsDfLastTwoMonths, method = "gcvEarth", trControl = trainControl(method = "cv", number = 10), tuneGrid = data.frame(degree = 1:5), preProcess = c("center", "scale"))

summary(mars_model)
# Visualize the MARS model
plot(mars_model)

# Get the best model
best_mars_model <- mars_model$finalModel
```

# Model combination
The final model combines the classification model and the regression model. We have created a function that accepts the test set, the classification model, and the regression model as input and returns the predicted values. The function first predicts the binary outcome using the random forest model. If the predicted outcome is non-zero, the function applies the regression model to predict the duration of calls. To make this function faster, we first predict the binary outcome for the entire test set and then apply the regression model only to the rows with positive outcomes. The other alternative was to apply the two models to each row of the test set, but this was much slower due to the way predict() works in R.

The threshold for the binary outcome is set to 0.25. This threshold was chosen based on the confusion matrix of the random forest model and it is set this low because it is better to have false positives that can still be predicted by the regression model than false negatives that will not be predicted by the regression model and will be set to zero.

After returning the predicted values, the script writes the results to a file.

```{r}
combinedModel <- function(test, classModel, regModel) {
  # Predict outcomes using the random forest model for the entire test set
  y_rf = predict(classModel, newdata = test, type = "prob")
  tresh = 0.25 
  y_rf = ifelse(y_rf[, 2] > tresh, 1, 0)

  # Initialize yhat with zeros
  yhat = rep(0, nrow(test))
  
  # Identify the rows where the prediction outcome is non-zero
  non_zero_indices = which(y_rf != 0)
  
  # Apply the regression model only to the rows with non-zero outcomes
  if(length(non_zero_indices) > 0) {
    yhat[non_zero_indices] = exp(predict(regModel, newdata = test[non_zero_indices, ]))-1
  }
  
  return(yhat)
}

# Predict all values of test set
yhat <- combinedModel(test, rfFit, mars_model)

# Write the results to a file
write.table(file="mySubmission.txt", pmax(0, yhat), row.names = FALSE, col.names = FALSE)
```

# Conclusions


