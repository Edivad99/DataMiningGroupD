---
title: "Rice Varieties Classification"
author: "Group D"
date: "2024-04-22"
output: html_document
---

```{r include=FALSE}
# Clear workspace, install library & import data
library("corrplot")
library("caret")
library("class")
library("Metrics")
library("MASS")
library("ggplot2")
library("gridExtra")
set.seed(1234)
```

## Importing Dataset (Task #1)
```{r}
rm(list = ls())
data <- read.csv("./dataset/rice_train.csv")
data$Class <- as.factor(data$Class)

trainIndex <- createDataPartition(data$Class,
                                  times = 1,
                                  p = .8, 
                                  list = FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]
summary(train)
```


## Data exploration (Task #2)

### Boxplots
```{r}
boxplot(train$Area ~ train$Class, xlab="Class", ylab = "Area")
boxplot(train$Perimeter ~ train$Class, xlab="Class", ylab = "Perimeter")
boxplot(train$Major_Axis_Length ~ train$Class, xlab="Class", ylab = "MajorAxisLength")
boxplot(train$Minor_Axis_Length ~ train$Class, xlab="Class", ylab = "MinorAxisLength")
boxplot(train$Eccentricity ~ train$Class, xlab="Class", ylab = "Eccentricity")
boxplot(train$Convex_Area ~ train$Class, xlab="Class", ylab = "ConvexArea")
boxplot(train$Extent ~ train$Class, xlab="Class", ylab = "EquivDiameter")


plot(train[,1:7])

cor(train[,1:7])
corrplot(cor(train[,1:7]), method = "number")
```

### Scatterplots
```{r}
plot(train[,], col=ifelse(train$Class == 1, "darkred", "lightblue"))
plot(train$Convex_Area, train$Area, type='p', col=ifelse(train$Class == 1, "darkred", "lightblue"), xlab = "ConvexArea", ylab = "Area")
```

```{r}
plot(train$Area, type='p', col="red", ylab = "Area vs Convex Area")
points(train$Convex_Area, type = "p", col="black")
```

```{r}
# Remove Area from train set
# Convex_Area is highly correlated with Area
train$Area <- NULL
```


## Performing LDA (Task #4)
```{r}
ldaModel <- lda(formula = Class ~ ., data = train)
ldaModel
```
With two classes (0/1), LDA produces only one discriminant function, which is sufficient to separate between the two classes in a one-dimensional space. Hence, the proportion of trace (percentage separations archived by LD1) is 100%
```{r}
predLda.train <- predict(ldaModel, newdata = train)
predLda.test <- predict(ldaModel, newdata = test)

# Determine how well the model fits
confusionMatrix(predLda.train$class, train$Class)
confusionMatrix(predLda.test$class, test$Class)
```
A Stacked Histogram can be used to see the class separation. The separation between the two classes is clear but with some overlap.
```{r}
ldahist(data = predLda.train$x, g=train$Class)
```

```{r}
# Try with cross validation leave one out
ldaModelCV <- lda(formula = Class ~ ., data = data, CV = TRUE)

confusionMatrix(ldaModelCV$class, data$Class)
```
Based on the Accuracy, the model that we chose is the one without CV.

## Performing QDA (Task #5)
```{r}
qdaModel <- qda(formula = Class ~ ., data = train)
qdaModel
```


##KNN
```{r}
#preproc

preProcValues <- preProcess(train, method = c("center", "scale"))
trainKNN <- predict(preProcValues, train)
testKNN <- predict(preProcValues, test)

knnModel <- train(
		     Class ~ ., 
		     data = trainKNN, 
	            method = "knn", 
		     trControl = trainControl(method = "cv"), 
		     tuneGrid = data.frame(k = seq(1,81, by=5))
		    )

best_model<- knn3(
                  Class ~ .,
                  data = trainKNN,
                  k = knnModel$bestTune$k
                 )

predictions <- predict(best_model, testKNN, type = "class")
# Calculate confusion matrix
cm <- confusionMatrix(predictions, testKNN$Class)
cm


```