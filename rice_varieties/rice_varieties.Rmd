---
title: "Rice Varieties Classification"
author: "Group D"
date: "2024-04-22"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

-   **Davide Albiero** davide.albiero@studenti.unipd.it
-   **Nazanin Ghorbani** nazanin.ghorbani@studenti.unipd.it
-   **Raman Yudzeshka** raman.yudzeshka@studenti.unipd.it
-   **Niccol√≤ Zenaro** niccolo.zenaro@studenti.unipd.it
-   **Luca Marchiori** luca.marchiori.3@studenti.unipd.it
-   **Mohammad Khosravi** mohammad.khosravi.1@studenti.unipd.it


```{r include=FALSE}
# Clear workspace, install library & import data
library("corrplot")
library("caret")
library("class")
library("Metrics")
library("MASS")
library("ggplot2")
library("gridExtra")
library("klaR")
set.seed(1234)
```

## Importing Dataset (Task #1)
```{r echo=FALSE}
rm(list = ls())
data <- read.csv("./dataset/rice_train.csv")
data$Class <- as.factor(data$Class)
summary(data)
```
## Splitting the dataset (Task #3)
```{r}
trainIndex <- createDataPartition(data$Class,
                                  times = 1,
                                  p = .8, 
                                  list = FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]
```

## Data exploration (Task #2)

### Boxplots
```{r echo=FALSE}
#boxplot(train$Area ~ train$Class, xlab="Class", ylab = "Area")
#boxplot(train$Perimeter ~ train$Class, xlab="Class", ylab = "Perimeter")
#boxplot(train$Major_Axis_Length ~ train$Class, xlab="Class", ylab = "MajorAxisLength")
#boxplot(train$Minor_Axis_Length ~ train$Class, xlab="Class", ylab = "MinorAxisLength")
#boxplot(train$Eccentricity ~ train$Class, xlab="Class", ylab = "Eccentricity")
boxplot(train$Convex_Area ~ train$Class, xlab="Class", ylab = "ConvexArea")
#boxplot(train$Extent ~ train$Class, xlab="Class", ylab = "Extent")
```

### Correlation Plot
From the plot, we can see that the Convex Area is highly correlated with the Area, that means that they are "telling the same thing"
That why after exploring the dataset Area is the most promissing features to delete.
```{r echo=FALSE}
#plot(train[,1:7])
#cor(train[,1:7])
corrplot(cor(train[,1:7]), method = "number")
```

### Scatterplots
```{r}
plot(train[,], col=ifelse(train$Class == 1, "darkred", "lightblue"))
plot(train$Convex_Area, train$Area, type='p', col=ifelse(train$Class == 1, "darkred", "lightblue"), xlab = "ConvexArea", ylab = "Area")
```

```{r}
plot(train$Area, type='p', col="red", ylab = "Area vs Convex Area")
points(train$Convex_Area, type = "p", col="black")
```

We proceed to remove the Area feature from the dataset.
```{r}
train$Area <- NULL
dataWithoutArea <- data
dataWithoutArea$Area <- NULL
```


## Performing LDA (Task #4)
```{r}
ldaModel <- lda(formula = Class ~ ., data = train)
ldaModel
```
With two classes (0/1), LDA produces only one discriminant function, which is sufficient to separate between the two classes in a one-dimensional space. Hence, the proportion of trace (percentage separations archived by LD1) is 100%
```{r}
predLda.train <- predict(ldaModel, newdata = train)
predLda.test <- predict(ldaModel, newdata = test)

# Determine how well the model fits
confusionMatrix(predLda.train$class, train$Class)
confusionMatrix(predLda.test$class, test$Class)
```

```{r echo=FALSE}
#https://forum.edgeimpulse.com/t/test-data-set-higher-accuracy-than-training-data-set/8200
```


A Stacked Histogram can be used to see the class separation. The separation between the two classes is clear but with some overlap.
```{r}
ldahist(data = predLda.train$x, g=train$Class)
partimat(Class ~ ., data = test, method = "lda", plot.matrix = TRUE, col.correct='green', col.wrong='red')
```

```{r}
# Try with cross validation leave one out
ldaModelCV <- lda(formula = Class ~ ., data = dataWithoutArea, CV = TRUE)

confusionMatrix(ldaModelCV$class, dataWithoutArea$Class)
```
Based on the Accuracy, the model that we chose is the one without CV.

## Performing QDA (Task #5)
```{r}
qdaModel <- qda(formula = Class ~ ., data = train)
qdaModel
```

```{r}
predQda.train <- predict(qdaModel, newdata = train)
predQda.test <- predict(qdaModel, newdata = test)

# Determine how well the model fits
confusionMatrix(predQda.train$class, train$Class)
confusionMatrix(predQda.test$class, test$Class)
```

```{r}
partimat(Class ~ ., data = test, method = "qda", plot.matrix = TRUE, col.correct='green', col.wrong='red')
```

```{r}
# Try with cross validation leave one out
qdaModelCV <- qda(formula = Class ~ ., data = dataWithoutArea, CV = TRUE)

confusionMatrix(qdaModelCV$class, dataWithoutArea$Class)
```

## Performing Logistic Regression (Task #6)

With the logistic regression model, the performances of the models have been evaluated one at a time by excluding one of the lesser important features. First, area; minor axis; extent; then minor axis; and finally, just extent in the models. It can be seen from the results that simply using all features for logistic regression had a higher accuracy than when some of the features were dropped. So, the final model have the accuracy of 0.9537 using all the features.

```{r}
# Fitting the Logestic Regression model
glm.fits = glm(Class~ ., data = train, family = binomial)
# Making prediction on the new data
predictions <- predict(glm.fits, newdata = test, type = "response") 
# Convert probabilities to predicted classes (0 or 1)
predicted_classes <- ifelse(predictions > 0.5, 2, 1)
confusionMatrix(as.factor(predicted_classes), test$Class)
```

## Performing KNN (Task #7)

Centering involves subtracting the mean of each variable, while scaling involves dividing by the standard deviation, ensuring that all variables have a mean of zero and a standard deviation of one.
The K is tuned using a grid search ranging from 1 to 81 with a step size of 5. After the cross-validation, the best K is selected based on the accuracy.
```{r}
#preproc

preProcValues <- preProcess(train, method = c("center", "scale"))
trainKNN <- predict(preProcValues, train)
testKNN <- predict(preProcValues, test)

knnModel <- train(
  Class ~ ., 
	data = trainKNN,
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = seq(1,81, by=5)))

best_model<- knn3(
  Class ~ .,
  data = trainKNN,
  k = knnModel$bestTune$k)
print(paste("Best K:", knnModel$bestTune$k, sep=" "))

predictions <- predict(best_model, testKNN, type = "class")
# Calculate confusion matrix
confusionMatrix(predictions, testKNN$Class)
```