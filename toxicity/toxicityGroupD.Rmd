---
title: "Toxicity TApp Group D"
author: "Group D"
date: "2024-045-22"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

- **Davide Albiero** davide.albiero@studenti.unipd.it
- **Nazanin Ghorbani** nazanin.ghorbani@studenti.unipd.it
- **Raman Yudzeshka** raman.yudzeshka@studenti.unipd.it
- **Niccolò Zenaro** niccolo.zenaro@studenti.unipd.it
- **Luca Marchiori** luca.marchiori.3@studenti.unipd.it
- **Mohammad Khosravi** mohammad.khosravi.1@studenti.unipd.it

```{r include=FALSE}
# Load libraries
library(caret)
library(ggplot2)
library(corrplot)
library(mgcv)
library(tidyr)
library(dplyr)
library(ISLR2)
library(GGally)
#Clear the workspace
rm(list=ls())
```

## Data import
```{r}
# Load the training data
train <- read.csv("toxicity_train.csv", stringsAsFactors=F)
# Load the test data
test <- read.csv("toxicity_validation.csv", stringsAsFactors=F)
set.seed(123)
```

## Data Visualization
```{r}

ggpairs(train,
        upper = list(continuous = wrap("cor", size = 2.5, color = "blue")),
  lower = list(continuous = wrap("points", size = 0.8, alpha = 0.7, color = "red")),
  diag = list(continuous = wrap("densityDiag", fill = "lightblue", alpha = 0.7)),
  axisLabels = "show")
#TODO what's going on

corrplot(cor(train), method = 'number')

ggplot(train, aes(x =nN, y=LC50)) + geom_point()
ggplot(train, aes(x =H050, y=LC50)) + geom_point()
ggplot(train, aes(x =C040, y=LC50)) + geom_point()
```

### Task 3

For this task, we train the model multiple times by increasing at each train the polynomial degree (from 1 to 5). The RMSE of each model is calculated after the cross-validation. The cross validation is used to ensure that the calculated RMSE is a good quality and not biased measure.

The results show that the 2-degree polynomial is the one with the lowest RMSE. A plot that compares the RMSE and the degree of polynomial is shown. In the end we also plot the final model function on the data scatterplot.
```{r}
# Train a model by trying multiple polynomial degrees. Save the models and their cross-validated RMSEs in a list and choose the best model based on the RMSE.

degrees <- 1:7
models <- list()
rmses <- c()

fitControl <- trainControl(method = "cv", number = 20)

for (degree in degrees){
  # Build up the formula with the selected degree
  formula <- as.formula(paste("LC50 ~ poly(RDCHI, ", degree, ")", sep=""))
  
  # Train the model
  fit <- caret::train(formula, data=train, method="lm", trControl=fitControl, preProcess=c("center", "scale"))
  
  # Get the cross-validated RMSE
  fitRmse <- fit$results$RMSE
  # Save the model RMSE
  rmses <- c(rmses, fitRmse)
}

# Plot the flexibility of the model vs RMSE
ggplot(data.frame(degree=degrees, RMSE=rmses), aes(x=degree, y=RMSE)) + geom_point() + geom_line() + xlab("Degree of polynomial") + ylab("RMSE")

# Choose the best model
bestDegree <- degrees[which.min(rmses)]
bestDegree

# Train the best model again (just for the sake of plotting it)
bestFormula <- as.formula(paste("LC50 ~ poly(RDCHI, ", bestDegree, ")", sep=""))
bestFit <- lm(bestFormula, data=train, method="lm", trControl=fitControl)

# Plot the data and the best model
ggplot(train, aes(x=RDCHI, y=LC50)) + geom_point(size=0.5, color="#34495e") + geom_line(aes(y=predict(bestFit), color="#c0392b"), show.legend = FALSE)

```

### Task 4
```{r}
# let's use 30 DOF and compare step function and cubic spline
# then it'll be 30 knots for the step function and 26 for the cubic spline

knots <- quantile(train$RDCHI, probs = seq(0, 1, length.out = 28))

# Step function
train$RDCHI_step <- cut(train$RDCHI, breaks=30, labels=FALSE)

# Define the model formulas
step_model_formula <- LC50 ~ factor(RDCHI_step)

str_knots <- paste("c(", noquote(paste(knots[-c(1, length(knots))], collapse = ", ")), ")", sep = "")
spline_model_formula <- as.formula(paste("LC50 ~ splines::bs(RDCHI, knots = ", str_knots, ", degree = 3)", sep = ""))
spline_model_formula

# Define train control with 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

str(train)
# Fit the models and calculate RMSE using cross-validation
model_step <- train(step_model_formula, data = train, method = "lm", trControl = ctrl)
model_spline <- train(spline_model_formula, data = train, method = "lm", trControl = ctrl)

summary(model_spline)


# Predicting
train$predicted_LC50_spline <- predict(model_spline, newdata=train)
train$predicted_LC50_step <- predict(model_step, newdata=train)

# Print RMSE for both models
cat("RMSE for Step Function Model (CV): ", model_step$results$RMSE, "\n")
cat("RMSE for Cubic Spline Model (CV): ", model_spline$results$RMSE, "\n")


# Plot the results
ggplot() +
  # Train data
  geom_point(data=train, aes(x = RDCHI, y = LC50), alpha = 0.5) +
  # Step function predictions
  stat_summary_bin(data=train, aes(x = RDCHI, y = predicted_LC50_step), 
                   bins = 30, geom = "step", color = "red", size = 1, alpha = 0.8) +
  # Cubic spline predictions
  geom_line(data=train, aes(x = RDCHI, y = predicted_LC50_spline), 
            color = "blue", size = 1, alpha = 0.8) +
  labs(title = "Step Function vs Cubic Spline Regression", 
       x = "RDCHI", 
       y = "LC50") +
  theme_minimal()


```

### Task 5
First, we need to find the best degrees of freedom for the model. We use the cross-validation score to find the best degrees of freedom.
```{r}
max_df = 30
res <- list()
for (i in 2:max_df) {
  spline <- smooth.spline(train$RDCHI, train$LC50, df = i)
  # The CV score is often called “PRESS” (and labeled on print()), for 'PREdiction Sum of Squares'.
  # print(paste("df:", i, "Cross-validation score:", spline$cv.crit))
  res <- c(res, spline$cv.crit)
}
best_df <- which.min(res)
```
We plot the degrees of freedom on the x-axis and the cross-validation score on the y-axis. The best model is chosen based on the minimum cross-validation score.
```{r}
plot(2:max_df, res, type = "l", xlab = "Degrees of freedom", ylab = "Cross-validation score")
points(best_df, res[best_df], pch = 19, col = "red")
legend("topright", legend = c(paste("Best df:", best_df, sep = "")), col = c("red"), pch = 19)
```

We plot the data, the basis linear model and the chosen spline model with the best degrees of freedom.
The model is chosen based on the minimum cross-validation score.
```{r}
reg <- lm(LC50 ~ RDCHI, data=train)
plot(train$RDCHI, train$LC50, xlab="RDCHI", ylab="LC50", pch=20, col="black")
abline(reg, col="blue")
#fit a spline with x degrees of freedom
spline <- smooth.spline(train$RDCHI, train$LC50, df=which.min(res))
lines(spline, col="red")
legend("topright", legend=c("Linear", "Spline"), col=c("blue", "red"), lty=1)
```

### Task 6
```{r}
# Check the number of unique values in each predictor
sapply(train, function(x) length(unique(x)))

# Fit a GAM model using natural cubic splines with reduced knots for each predictor
gam_model <- gam(LC50 ~ 
                   s(TPSA, bs = "cs", k = 20) +   # Less than 193
                   s(SAacc, bs = "cs", k = 20) +  # Less than 182
                   s(H050, bs = "cs", k = 9) +    # Less than 10
                   s(MLOGP, bs = "cs", k = 20) +  # Less than 349 (k can be higher but 20 is reasonable)
                   s(RDCHI, bs = "cs", k = 20) +  # Less than 297 (k can be higher but 20 is reasonable)
                   s(GATS1p, bs = "cs", k = 20) + # Less than 349 (k can be higher but 20 is reasonable)
                   s(nN, bs = "cs", k = 9) +      # Less than 9
                   s(C040, bs = "cs", k = 6),     # Less than 6
                 data = train)

# Summarize the model
summary(gam_model)

# Extract partial effects of each predictor
partial_effects <- predict(gam_model, type = "terms", se.fit = TRUE)
partial_effects_df <- as.data.frame(partial_effects$fit)
colnames(partial_effects_df) <- gsub("\\.s\\(.*\\)", "", colnames(partial_effects_df))
partial_effects_df$LC50 <- train$LC50

# Convert the data to long format for ggplot
partial_effects_long <- partial_effects_df %>%
  pivot_longer(cols = -LC50, names_to = "Predictor", values_to = "Effect")

# Plot the effects of each predictor using ggplot2
ggplot(partial_effects_long, aes(x = Effect, y = LC50)) +
  geom_point() +
  facet_wrap(~ Predictor, scales = "free_x") +
  labs(title = "Partial Effects of Predictors on LC50 using natural cubic splines",
       x = "Partial Effect",
       y = "LC50") +
  theme_minimal()
```
