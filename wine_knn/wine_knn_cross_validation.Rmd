---
title: "Implementation of KNN and Cross-Validation on the Wine Quality dataset"
author: "Group D"
date: "2024-04-07"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

-   **Davide Albiero** davide.albiero@studenti.unipd.it
-   **Nazanin Ghorbani** nazanin.ghorbani@studenti.unipd.it
-   **Raman Yudzeshka** raman.yudzeshka@studenti.unipd.it
-   **Niccol√≤ Zenaro** niccolo.zenaro@studenti.unipd.it
-   **Luca Marchiori** luca.marchiori.3@studenti.unipd.it
-   **Mohammad Khosravi** mohammad.khosravi.1@studenti.unipd.it

## Abstract

In this report we summarize our attempt of implementing the k-nearest neighbors (KNN) model to the Wine Quality dataset, using cross-validation. Our aim is to use cross validation to find an optimal value of k, the number of neighbors considered in KNN, to minimize prediction errors. In the end, we plot RMSE based on train data and cross-validation as a function of 1/K (K of KNN) to highlight the best K value and how the RMSE relates to the selected K.

## Dataset

```{r include=FALSE}
# Clear workspace, install library & import data
library("caret")
library("Metrics")
set.seed(1234)

rm(list = ls())

data.trn <- read.csv("wineq_train.csv", stringsAsFactors=F)
```

The dataset consists of train set with 3698 obs. of 12 variables as below:

**Input variables** (based on physicochemical tests):

1.  fixed acidity
2.  volatile acidity
3.  citric acid
4.  residual sugar
5.  chlorides
6.  free sulfur dioxide
7.  total sulfur dioxide
8.  density
9.  pH
10. sulphates
11. alcohol

**Output variable** (based on sensory data):

12. quality (score between 0 and 10)

```{r echo=FALSE}
str(data.trn)
```

## Results
We begin by importing the wine quality dataset and setting up a 10-fold cross-validation using the `caret` package.
The KNN algorithm is used for modeling, with the dataset preprocessed by centering and scaling to ensure standardized features.

Since there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation, we have chosen k=10. This is because using k = 5 or k = 10 has been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.

To identify the optimal value of k, we conduct a grid search over a range of k values, ranging from 5 to 70 with increments of 5.
For each k, we perform cross-validation and calculate the root mean squared error (RMSE) as a measure of model performance.

We add centering and scaling preprocessing to the model. Centering involves subtracting the mean of each predictor variable from all of its values. Scaling involves dividing each predictor variable by its standard deviation. Since KNN is a distance-based model, centering and scaling ensure that all variables contribute equally to the model calculation.

The KNN model is trained for each number of K from 5 to 70 (with a step size of 5). The train function executes a grid search to assests the best K on the provided range.

```{r}
## Calculate RMSE for different values of k
ctrl <- trainControl(method="cv", number=10) #10 fold cross validation

K <- seq(5, 70, by=5)

cv_rmse <- rep(NA, length(K))
best_k <- 0
best_k_index <- 1

train_rmse <- rep(NA, length(K))
i <- 1
for (k in K){
  model <- train(quality ~ .,
                 method = "knn",
                 trControl = ctrl,
                 preProcess = c("center","scale"),
                 tuneGrid = data.frame(k=k),
                 data = data.trn)

  if (i==1 || cv_rmse[best_k_index] > model$results$RMSE){
    best_k <- k
    best_k_index <- i
  }
  cv_rmse[i] <- model$results$RMSE
  predictions_train <- predict(model, newdata = data.trn)
  train_rmse[i] <- rmse(data.trn$quality, predictions_train)
  i <- i + 1
}
```

The experiments involve iteratively training KNN models with different values of k and evaluating their performance through cross-validation.

We plot the relationship between 1/k (where k is the number of neighbors) and the cross-validation RMSE and training RMSE to visualize how model complexity impacts prediction accuracy.
Additionally, we compare the training RMSE with the cross-validation RMSE to assess overfitting.
Finally, we highlighted in the graph the best k in the RMSE cross-validation results.

```{r echo=FALSE}
## Plot RMSE vs 1/k
plot(1/K, cv_rmse, type = "l", xlab = "1/k", ylab = "RMSE", main = "RMSE vs 1/k", col="red", ylim=c(0.71,0.73))

index = best_k_index
points(1/best_k, cv_rmse[index], pch = "o", col = "black")
legend("topright", legend = c("CV RMSE", paste("Best result k=", best_k)), col = c("red", "black"), pch = 16)
```


```{r}
## Plot RMSE vs 1/k
plot(1/K, cv_rmse, type = "l", xlab = "1/k", ylab = "RMSE", main = "RMSE vs 1/k", col="red", ylim=c(0.56,0.73))
points(1/K, train_rmse, type = "l", pch = 16, col = "blue")

index = best_k_index
points(1/best_k, cv_rmse[index], pch = "o", col = "black")
legend("right", legend = c("Training RMSE", "CV RMSE", paste("Best result k=", best_k)), col = c("blue", "red", "black"), pch = 16)
```

## Conclusion
In conclusion, our experimentation demonstrates the effectiveness of KNN coupled with cross-validation in predicting wine quality.
We find that the model's performance varies with the choice of k, highlighting the importance of hyperparameter tuning. Through systematic evaluation and visualization, we identify the optimal k value of 10, achieving the best cross-validated RMSE.
This suggests that considering a moderate number of nearest neighbors leads to a well-generalized model for predicting wine quality.
Overall, our study underscores the significance of cross-validation in model selection and parameter tuning, providing valuable insights for building robust predictive models in similar domains.